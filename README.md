# trabalho05_eEDB_011

Tarefa 5 - Aula 6
Implementar  a atividades da tarefa 3  ou da tarefa 5 (escolher uma das duas tarefas) com as seguintes diferen√ßas:

O processamento realizado deve ser aplicado em um servi√ßo Serverless***

A fonte de dados dever ser as seguintes:

1 -  Ranking de Institui√ß√µes por √çndice de Reclama√ß√µes (Arquivos CSV)

2 - Tarifas Banc√°rias - por Segmento e por Institui√ß√£o (Dados em uma base de dados Relacional)

Camadas de Dados no AWS S3:

RAW (Dados Crus)
Trusted (Dados Tratados mas sem implementa√ß√£o de modelagem)
Analytics (Modelado em Star Schema - Replicados em base de dados relacional)
Camadas Trusted e Analytics devem ser mapeadas no AWS Glue Catalog

Implementar Airflow e DBT √© opcional

## üöÄ Come√ßando

Trabalho 05 ministrado pelo professor Leandro Mendes Ferreira no segundo semestre de 2022 - Ingest√£o de Dados.

Defininos as seguintes camadas: 

- `Raw`: Pasta para dados brutos
- `Raw_scripts`: Pasta de C√≥digos para dados brutos
- `Trusted`: Pasta para Dados tratados
- `Trusted_scripts`: Pasta de C√≥digos para Dados tratados
- `Refined`: Pasta para Dados Dados tratados e modelados
- `Refined_scripts`: Pasta de C√≥digos para Dados Dados tratados e modelados
- `source`: Dados para extra√ß√£o
- `drivers`: jar para execu√ß√£o de inser√ß√£o Spark 


## üìã Implementa√ß√£o
<img src="./images/arquitetura.jpg" width="75%">

* 1 - Nossa estrat√©gia foi recriar a estrutura de camadas exigidas para as atividades anteriores dentro do S3. Entendemos que o mais adequado seria criar um bucket para cada camada, mas como os c√≥digo j√° haviam sido modificados para um √∫nico bucket, mantivemos essa estrutura. 

* 2 - Como dessa vez n√£o poder√≠amos utilizar um cluster gerenci√°vel, nossa primeira estrat√©gia foi utilizar o AWS Glue para a execu√ß√£o dos scripts.
No Glue, poder√≠amos adotar uma estrat√©gia de subir um script Python em notebook e execut√°-lo, ou submeter um script Spark. Criamos um Job para cada estrat√©gia.
<img src="./images/2-glue.jpg" width="75%">

* 3 - Cada job criou os resultados no S3 e na base do Mysql.
<img src="./images/3_jobs_glue.png" width="75%">

* 4 - Tamb√©m tentamos viabilizar a execu√ß√£o do Spark no Lambda. Por√©m, em virtude das limita√ß√µes do Lambda (tamanho m√°ximo de arquivo para submeter), n√£o conseguimos rodar a fun√ß√£o necess√°ria. Nossa estrat√©gia inicial de utilizar Pyspak se provou um dificultador para a realiza√ß√£o das demais tarefas em compara√ß√£o √† um c√≥digo implementado com Python puro.
<img src="./images/4 - lambda.png" width="75%">
 
* 5 - Base MySQL adquirido do provedor UOL. Reaproveitamos da atividade anterior


* 6 - Grafana para visualiza√ß√£o dos dados
 <img src="./images/8-GrafanaGraficos.jpg" width="75%">
 
* 7 - MySQL Workbench para verifica√ß√£o do processo de inser√ß√£o de dados na base uol.
 <br>
 * 8 - AWS Glue para documenta√ß√£o do cat√°logo de dados.
 Para gerar a documenta√ß√£o do cat√°logo de dados seguimos o seguinte processo:
 <br> Criar duas bases de dados, uma para cada reposit√≥rio do S3 que quer√≠amos documentar (trusted e refined)
 <br> Configurar dois crawlers para mapear a estruturas dos dados de cada reposit√≥rio.
 <br> Iniciar os crawlers que geraram as respectivas tabelas de metadados de cada reposit√≥rio



<hr>

## Resultados
Acreditamos que atendemos todos os requisitos obrigat√≥rios propostos para a tarefa. 
<br> Tivemos problemas para criar uma aplica√ß√£o serveless para rodar Spark no Lambda, mas a execu√ß√£o no Glue ocorreu como esperado.

## üõ†Ô∏è Constru√≠do com
* [Python](https://www.python.org/) - Linhas de c√≥digo utilizado para programa√ß√£o;
* [PySpark](https://spark.apache.org/docs/latest/api/python/) - Utilizado para ETL dos dados;
* [MySQL](https://www.mysql.com/) - Utilizado para ETL dos dados;
* [AWS S3](https://aws.amazon.com/pt/s3/) - Utilizado como reposit√≥rio de dados;
* [AWS Glue](https://https://aws.amazon.com/pt/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc/) - Utilizado para mapeamento do cat√°logo de dados;

## ‚úíÔ∏è Autores
* [Rodrigo Vitorino](https://github.com/digaumlv)
* [Thais Nabe](https://github.com/thaisnabe)
* [Vitor Marques](https://github.com/vitormrqs)
* [Wesley Louren√ßo Barbosa](https://github.com/wesleyloubar)

