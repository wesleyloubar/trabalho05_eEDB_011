{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "\n",
    "# Glue Studio Notebook\n",
    "You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
    "\n",
    "## Available Magics\n",
    "|          Magic              |   Type       |                                                                        Description                                                                        |\n",
    "|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
    "| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
    "| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
    "| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
    "| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
    "| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
    "| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
    "| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
    "| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
    "| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
    "| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
    "| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0                                                        |\n",
    "| %security_configuration     |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
    "| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
    "| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
    "| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
    "| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
    "| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
    "| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#tratar os dados da api\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "url = 'jdbc:mysql://db-ingestao04.mysql.uhserver.com/db_ingestao04'\n",
    "password = '*heybancoUSP2'\n",
    "user = 'user_ingestao04'\n",
    "\n",
    "def write_mysql(df,table):\n",
    "\n",
    "\n",
    "    df. write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .save()\n",
    "\n",
    "def read_mysql(table,spark):\n",
    "\n",
    "\n",
    "    return spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH_RAW = 's3://ingestao04/raw/reclamacao/'\n",
    "PATH_TRUSTED = 's3://ingestao04/trusted/reclamacao/'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TRUSTED_CSV\").getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.parquet(PATH_RAW)\n",
    "df = df.drop('C14')\n",
    "\n",
    "#TYPE\n",
    "df = df.withColumn('ANO',F.col('ANO').cast('int'))\n",
    "\n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES',F.col('QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES').cast('bigint'))                                                 \n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS',F.col('QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS').cast('bigint'))                                                  \n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_NO_REGULADAS',F.col('QUANTIDADE_DE_RECLAMAES_NO_REGULADAS').cast('bigint'))                                                      \n",
    "df = df.withColumn('QUANTIDADE_TOTAL_DE_RECLAMAES',F.col('QUANTIDADE_TOTAL_DE_RECLAMAES').cast('bigint'))                                                  \n",
    "df = df.withColumn('QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR',F.col('QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR').cast('bigint'))                                                 \n",
    "df = df.withColumn('QUANTIDADE_DE_CLIENTES_CCS',F.col('QUANTIDADE_DE_CLIENTES_CCS').cast('bigint'))                                               \n",
    "df = df.withColumn('QUANTIDADE_DE_CLIENTES_SCR',F.col('QUANTIDADE_DE_CLIENTES_SCR').cast('bigint'))\n",
    "\n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES',F.coalesce(F.col('QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES'),F.lit(0)))                                                 \n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS',F.coalesce(F.col('QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS'),F.lit(0)))                                                    \n",
    "df = df.withColumn('QUANTIDADE_DE_RECLAMAES_NO_REGULADAS',F.coalesce(F.col('QUANTIDADE_DE_RECLAMAES_NO_REGULADAS'),F.lit(0)))                                                       \n",
    "df = df.withColumn('QUANTIDADE_TOTAL_DE_RECLAMAES',F.coalesce(F.col('QUANTIDADE_TOTAL_DE_RECLAMAES'),F.lit(0)))                                                 \n",
    "df = df.withColumn('QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR',F.coalesce(F.col('QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR'),F.lit(0)))                                                 \n",
    "df = df.withColumn('QUANTIDADE_DE_CLIENTES_CCS',F.coalesce(F.col('QUANTIDADE_DE_CLIENTES_CCS'),F.lit(0)))                                               \n",
    "df = df.withColumn('QUANTIDADE_DE_CLIENTES_SCR',F.coalesce(F.col('QUANTIDADE_DE_CLIENTES_SCR'),F.lit(0)))\n",
    "\n",
    "#descastando CNPJ nullo\n",
    "\n",
    "df = df.where(F.col('CNPJ_IF').cast('bigint').isNotNull())\n",
    "\n",
    "df.write.mode('overwrite').parquet(PATH_TRUSTED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.file_csv_trusted2'\n",
    "write_mysql(df,TABLE_MYSQL)\n",
    "print('rodei file_csv_trusted2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python_glue_session"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ANO: int, TRIMESTRE: string, CATEGORIA: string, TIPO: string, CNPJ_IF: string, INSTITUIO_FINANCEIRA: string, NDICE: string, QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES: bigint, QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS: bigint, QUANTIDADE_DE_RECLAMAES_NO_REGULADAS: bigint, QUANTIDADE_TOTAL_DE_RECLAMAES: bigint, QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR: bigint, QUANTIDADE_DE_CLIENTES_CCS: bigint, QUANTIDADE_DE_CLIENTES_SCR: bigint]\n"
     ]
    }
   ],
   "source": [
    "PATH_RAW = 's3://ingestao04/raw/api_reclamacao/'\n",
    "PATH_TRUSTED = 's3://ingestao04/trusted/api_reclamacao/'\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"TRUSTED_API\").config(\"spark.jars\", \"/workspaces/trabalho03_eEDB_011/drives/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"TRUSTED_API\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(PATH_RAW)\n",
    "df = df.withColumn('value_2',F.explode(df.value))\n",
    "\n",
    "#FLAT\n",
    "df = df.withColumn('CODIGOSERVICO', F.col('value_2').CodigoServico)\n",
    "df = df.withColumn('DATAVIGENCIA', F.col('value_2').DataVigencia)\n",
    "df = df.withColumn('PERIODICIDADE', F.col('value_2').Periodicidade)\n",
    "df = df.withColumn('SERVICO', F.col('value_2').Servico)\n",
    "df = df.withColumn('TIPOVALOR', F.col('value_2').TipoValor)\n",
    "df = df.withColumn('UNIDADE', F.col('value_2').Unidade)\n",
    "df = df.withColumn('VALORMAXIMO', F.col('value_2').ValorMaximo)\n",
    "\n",
    "#TYPE\n",
    "df = df.withColumn('DATAVIGENCIA', F.to_date(F.col('DATAVIGENCIA'), 'yyyy-MM-dd'))\n",
    "df = df.withColumn('VALORMAXIMO', F.col('VALORMAXIMO').cast('float'))\n",
    "df = df.withColumnRenamed('cnpj', 'CNPJ')\n",
    "df = df.select('CNPJ','CODIGOSERVICO','DATAVIGENCIA','PERIODICIDADE','SERVICO','TIPOVALOR','UNIDADE', 'VALORMAXIMO')\n",
    "\n",
    "df.write.mode('overwrite').parquet(PATH_TRUSTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.api_trusted2'\n",
    "write_mysql(df,TABLE_MYSQL)\n",
    "print('rodei api_trusted2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRUSTED = 's3://ingestao04/trusted/reclamacao/' \n",
    "PATH_SINK = 's3://ingestao04/refined/DM_CATEGORIA.parquet'\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"REFINED_CAT\").config(\"spark.jars\", \"/workspaces/trabalho03_eEDB_011/drives/mysql-connector-java-8.0.22.jar\").getOrCreate() spark = SparkSession.builder.appName(\"REFINED_CAT\").getOrCreate()\n",
    "\n",
    "spark.read.parquet(PATH_TRUSTED).createOrReplaceTempView('file_csv')\n",
    "\n",
    "df = spark.sql(''' select monotonically_increasing_id() as ID_CAT, CATEGORIA from (select distinct upper(CATEGORIA) as CATEGORIA from file_csv) ''')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.write.mode('overwrite').parquet(PATH_SINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.DM_CATEGORIA2' \n",
    "write_mysql(df,TABLE_MYSQL) \n",
    "print('rodei DM_CATEGORIA2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRUSTED = 's3://ingestao04/trusted/reclamacao/'\n",
    "PATH_SINK = 's3://ingestao04/refined/DM_INDICE.parquet'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"REFINED_IND\").config(\"spark.jars\", \"/workspaces/trabalho03_eEDB_011/drives/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "\n",
    "spark.read.parquet(PATH_TRUSTED).createOrReplaceTempView('file_csv')\n",
    "\n",
    "df = spark.sql('''\n",
    "    select monotonically_increasing_id() as ID_IND, INDICE FROM (select distinct COALESCE(upper(NDICE),-1) as INDICE from file_csv WHERE ISNOTNULL(NDICE))\n",
    "    ''')\n",
    "\n",
    "df.show()\n",
    "df.write.mode('overwrite').parquet(PATH_SINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.DM_INDICE2' \n",
    "write_mysql(df,TABLE_MYSQL) \n",
    "print('rodei DM_INDICE2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRUSTED = 's3://ingestao04/trusted/reclamacao/'\n",
    "PATH_SINK = 's3://ingestao04/refined/DM_INSTITUICAO.parquet'\n",
    "\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"REFINED_INST\").config(\"spark.jars\", \"s3://trabalho04/drives/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "#spark = SparkSession.builder.appName(\"REFINED_INST\").config(\"spark.jars\", \"/home/hadoop/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"REFINED_INST\").getOrCreate()\n",
    "\n",
    "\n",
    "spark.read.parquet(PATH_TRUSTED).createOrReplaceTempView('file_csv')\n",
    "\n",
    "df = spark.sql('''\n",
    "    select monotonically_increasing_id() as ID_INST, INSTITUIO_FINANCEIRA,CNPJ_IF FROM (select distinct UPPER(INSTITUIO_FINANCEIRA) AS INSTITUIO_FINANCEIRA, CNPJ_IF from file_csv)\n",
    "    ''')\n",
    "\n",
    "df.show()\n",
    "df.write.mode('overwrite').parquet(PATH_SINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.DM_INSTITUICAO2'\n",
    "write_mysql(df,TABLE_MYSQL)\n",
    "print('rodei DM_INSTITUICAO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRUSTED = 's3://ingestao04/trusted/reclamacao/'\n",
    "PATH_SINK = 's3://ingestao04/refined/DM_TIPO.parquet'\n",
    "\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"REFINED_TIP\").config(\"spark.jars\", \"/workspaces/trabalho03_eEDB_011/drives/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"REFINED_TIP\").getOrCreate()\n",
    "\n",
    "\n",
    "spark.read.parquet(PATH_TRUSTED).createOrReplaceTempView('file_csv')\n",
    "\n",
    "df = spark.sql('''\n",
    "    select monotonically_increasing_id() as ID_IP, TIPO FROM (select distinct UPPER(TIPO) AS TIPO from file_csv)\n",
    "    ''')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.write.mode('overwrite').parquet(PATH_SINK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.DM_TIPO2'\n",
    "write_mysql(df,TABLE_MYSQL)\n",
    "print('rodei DM_TIPO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRUSTED_API = 's3://ingestao04/trusted/api_reclamacao/'\n",
    "PATH_TRUSTED_FATO = 's3://ingestao04/trusted/reclamacao/'\n",
    "\n",
    "PATH_SINK = 's3://ingestao04/refined/FT_INDICE_RECLAMACAO.parquet'\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"REFINED_FT\").config(\"spark.jars\", \"/workspaces/trabalho03_eEDB_011/drives/mysql-connector-java-8.0.22.jar\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"REFINED_FT\").getOrCreate()\n",
    "\n",
    "spark.read.parquet(PATH_TRUSTED_API).createOrReplaceTempView('API_RECLAMACAO')\n",
    "spark.read.parquet(PATH_TRUSTED_FATO).createOrReplaceTempView('RECLAMACAO')\n",
    "read_mysql('db_ingestao04.DM_CATEGORIA',spark).createOrReplaceTempView('DM_CATEGORIA')\n",
    "read_mysql('db_ingestao04.DM_TIPO',spark).createOrReplaceTempView('DM_TIPO')\n",
    "read_mysql('db_ingestao04.DM_INDICE',spark).createOrReplaceTempView('DM_INDICE')\n",
    "read_mysql('db_ingestao04.DM_INSTITUICAO',spark).createOrReplaceTempView('DM_INSTITUICAO')\n",
    "\n",
    "df = spark.sql('''\n",
    "    SELECT\n",
    "        RECLAMACAO.ANO,\n",
    "        RECLAMACAO.TRIMESTRE,\n",
    "        DM_CATEGORIA.ID_CAT,\n",
    "        DM_TIPO.ID_IP,\n",
    "        DM_INSTITUICAO.ID_INST,\n",
    "        DM_INDICE.ID_IND,\n",
    "        RECLAMACAO.QUANTIDADE_DE_RECLAMAES_REGULADAS_PROCEDENTES,\n",
    "        RECLAMACAO.QUANTIDADE_DE_RECLAMAES_REGULADAS_OUTRAS,\n",
    "        RECLAMACAO.QUANTIDADE_DE_RECLAMAES_NO_REGULADAS,\n",
    "        RECLAMACAO.QUANTIDADE_TOTAL_DE_RECLAMAES,\n",
    "        RECLAMACAO.QUANTIDADE_TOTAL_DE_CLIENTES_CCS_E_SCR,\n",
    "        RECLAMACAO.QUANTIDADE_DE_CLIENTES_CCS,\n",
    "        RECLAMACAO.QUANTIDADE_DE_CLIENTES_SCR,\n",
    "        API_RECLAMACAO.VALORMAXIMO,\n",
    "        API_RECLAMACAO.DATAVIGENCIA\n",
    "    FROM RECLAMACAO\n",
    "    LEFT JOIN API_RECLAMACAO ON RECLAMACAO.CNPJ_IF = API_RECLAMACAO.CNPJ\n",
    "    LEFT JOIN DM_CATEGORIA ON UPPER(RECLAMACAO.CATEGORIA) = UPPER(DM_CATEGORIA.CATEGORIA)\n",
    "    LEFT JOIN DM_TIPO ON UPPER(RECLAMACAO.TIPO) = UPPER(DM_TIPO.TIPO)\n",
    "    LEFT JOIN DM_INDICE ON UPPER(RECLAMACAO.NDICE) = UPPER(DM_INDICE.INDICE)\n",
    "    LEFT JOIN DM_INSTITUICAO ON UPPER(RECLAMACAO.INSTITUIO_FINANCEIRA) = UPPER(DM_INSTITUICAO.INSTITUIO_FINANCEIRA) AND RECLAMACAO.CNPJ_IF = DM_INSTITUICAO.CNPJ_IF\n",
    "    WHERE TRIM(UPPER(API_RECLAMACAO.TIPOVALOR)) == 'REAL'\n",
    "    ''')\n",
    "\n",
    "df.write.mode('overwrite').parquet(PATH_SINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_MYSQL = 'db_ingestao04.FT_INDICE_RECLAMACAO2'\n",
    "write_mysql(df,TABLE_MYSQL)\n",
    "print('rodei FT_INDICE_RECLAMACAO2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
